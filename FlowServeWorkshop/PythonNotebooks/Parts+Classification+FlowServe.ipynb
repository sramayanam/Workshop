{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# USE SPARK MLLIB & SPARK MSFT MLLIB\n",
    "********************FLOW SERVE CLASSIFICATION MODEL BASED ON SAMPLE DATA ***********************\n",
    "***********INPUT FILE, SCHEMA & DETAILS : parts_classification.data.csv*************************\n",
    "PartCode -> string -> It is the part number/code\n",
    "OrganizationKey -> integer -> range 1 – 5\n",
    "An identifier for a part of Flowserve which combined with the PartCode produces a unique id for the part\n",
    "ItemDescription -> string -> the free text description of the part; this is what is used to classify the part\n",
    "EuropumpBubbleNumber -> string -> the target to predict from the ItemDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, urllib\n",
    "import pandas as pd\n",
    "import mmlspark\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***********Import File, Parse the fields & prepare label for classification********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f8827c5b0f0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#dataFile = \"adl://srramhdi.azuredatalakestore.net/clusters/srramhdir/HdiNotebooks/sample.csv\"\n",
    "textSchema = StructType([StructField(\"partcode\", StringType(), False),\n",
    "                         StructField(\"orgkey\", StringType(), False),\n",
    "                         StructField(\"itemdesc\",StringType(), False),\n",
    "                         StructField(\"europumpbubbleno\", StringType(), False)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File b'adl://srramhdi.azuredatalakestore.net/Files/sample.csv' does not exist\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\", line 562, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\", line 315, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\", line 645, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\", line 799, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/pandas/io/parsers.py\", line 1213, in __init__\n",
      "    self._reader = _parser.TextReader(src, **kwds)\n",
      "  File \"pandas/parser.pyx\", line 358, in pandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\n",
      "  File \"pandas/parser.pyx\", line 628, in pandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\n",
      "OSError: File b'adl://srramhdi.azuredatalakestore.net/Files/sample.csv' does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame(pd.read_csv(\"adl://srramhdi.azuredatalakestore.net/Files/sample.csv\", sep=\",\", header=None), textSchema)\n",
    "data.registerTempTable(\"data1\")\n",
    "df = spark.sql(\"SELECT itemdesc,CASE WHEN cast(europumpbubbleno as int) = 2200 THEN 1 \\\n",
    "                 WHEN cast(europumpbubbleno as int) = 1100 THEN 2 \\\n",
    "               ELSE 0 END as label FROM data1\")\n",
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ******Using plain old Spark ML Lib********************************\n",
    "******STEP 1 : CREATE FEATURE VECTOR FROM FREE FORM TEXT**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemdesc</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenizedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASING</td>\n",
       "      <td>0</td>\n",
       "      <td>[casing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASING</td>\n",
       "      <td>0</td>\n",
       "      <td>[casing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>???BRG|SS400|[*AN12 D80 T11 2]|????1||H11|?????</td>\n",
       "      <td>0</td>\n",
       "      <td>[???brg|ss400|[*an12, d80, t11, 2]|????1||h11|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIPE OF THE SINGLE SCREW</td>\n",
       "      <td>0</td>\n",
       "      <td>[pipe, of, the, single, screw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>???????|SUS304TP|[*PT1/2X115L SCH80 0]|????1||...</td>\n",
       "      <td>0</td>\n",
       "      <td>[???????|sus304tp|[*pt1/2x115l, sch80, 0]|????...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>???????|SUS316TP|[*PT3/4X140L SCH80 0]|????1||...</td>\n",
       "      <td>0</td>\n",
       "      <td>[???????|sus316tp|[*pt3/4x140l, sch80, 0]|????...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>???????|SUS316TP|[*PT3/4X400L SCH80 0]|????1||...</td>\n",
       "      <td>0</td>\n",
       "      <td>[???????|sus316tp|[*pt3/4x400l, sch80, 0]|????...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PIPE OF THE SINGLE SCREW</td>\n",
       "      <td>0</td>\n",
       "      <td>[pipe, of, the, single, screw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>???????|STPT370|[*NPT1/2X60L SCH80]|????1||H11...</td>\n",
       "      <td>0</td>\n",
       "      <td>[???????|stpt370|[*npt1/2x60l, sch80]|????1||h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>????|SS400||????1||H11|?????</td>\n",
       "      <td>0</td>\n",
       "      <td>[????|ss400||????1||h11|?????]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            itemdesc  label  \\\n",
       "0                                             CASING      0   \n",
       "1                                             CASING      0   \n",
       "2    ???BRG|SS400|[*AN12 D80 T11 2]|????1||H11|?????      0   \n",
       "3                           PIPE OF THE SINGLE SCREW      0   \n",
       "4  ???????|SUS304TP|[*PT1/2X115L SCH80 0]|????1||...      0   \n",
       "5  ???????|SUS316TP|[*PT3/4X140L SCH80 0]|????1||...      0   \n",
       "6  ???????|SUS316TP|[*PT3/4X400L SCH80 0]|????1||...      0   \n",
       "7                           PIPE OF THE SINGLE SCREW      0   \n",
       "8  ???????|STPT370|[*NPT1/2X60L SCH80]|????1||H11...      0   \n",
       "9                       ????|SS400||????1||H11|?????      0   \n",
       "\n",
       "                                       tokenizedText  \n",
       "0                                           [casing]  \n",
       "1                                           [casing]  \n",
       "2  [???brg|ss400|[*an12, d80, t11, 2]|????1||h11|...  \n",
       "3                     [pipe, of, the, single, screw]  \n",
       "4  [???????|sus304tp|[*pt1/2x115l, sch80, 0]|????...  \n",
       "5  [???????|sus316tp|[*pt3/4x140l, sch80, 0]|????...  \n",
       "6  [???????|sus316tp|[*pt3/4x400l, sch80, 0]|????...  \n",
       "7                     [pipe, of, the, single, screw]  \n",
       "8  [???????|stpt370|[*npt1/2x60l, sch80]|????1||h...  \n",
       "9                     [????|ss400||????1||h11|?????]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "tokenizer = Tokenizer(inputCol=\"itemdesc\", outputCol=\"tokenizedText\")\n",
    "tokenizedData = tokenizer.transform(df)\n",
    "numFeatures = 1000\n",
    "hashingScheme = HashingTF(inputCol=\"tokenizedText\",\n",
    "                          outputCol=\"features\",\n",
    "                          numFeatures=numFeatures)\n",
    "featurizedData = hashingScheme.transform(tokenizedData)\n",
    "processedData = featurizedData.withColumn(\"label\", featurizedData[\"label\"]) \\\n",
    "                             .select([\"features\", \"label\"])\n",
    "#featurizedData.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Trying three different models from Spark MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numNodes =  13\n",
      "depth =  3\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Prepare data for learning\n",
    "train, test, validation = processedData.randomSplit([0.60, 0.20, 0.20], seed=123)\n",
    "\n",
    "#Train the models on the 'train' data\n",
    "lrHyperParams = [0.05, 0.1, 0.2, 0.4]\n",
    "logisticRegressions = [LogisticRegression(regParam = hyperParam)\n",
    "                       for hyperParam in lrHyperParams]\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "metrics = []\n",
    "models = []\n",
    "\n",
    "# Select the best model\n",
    "for learner in logisticRegressions:\n",
    "    model = learner.fit(train)\n",
    "    models.append(model)\n",
    "    scored_data = model.transform(test)\n",
    "    metrics.append(evaluator.evaluate(scored_data))\n",
    "best_metric = max(metrics)\n",
    "best_model = models[metrics.index(best_metric)]\n",
    "\n",
    "# Save model\n",
    "best_model.write().overwrite().save(\"sparkmlflowserveclassification.mmls\")\n",
    "# Get AUC on the validation dataset\n",
    "scored_val = best_model.transform(validation)\n",
    "print(evaluator.evaluate(scored_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978040890743592"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Build a Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "dtModel = dt.fit(train)\n",
    "print (\"numNodes = \", dtModel.numNodes)\n",
    "print (\"depth = \", dtModel.depth)\n",
    "# Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "predictions = dtModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tuning model Hyper Parameters for decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Tune Hyper parameters for Decision Tree model \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1,2,6,10])\n",
    "             .addGrid(dt.maxBins, [20,40,80])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(train)\n",
    "# Evaluate model\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974381737296761"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an initial RandomForest model.\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Using MML Spark 1.Eliminate lot of boiler plate code/ Feauturize efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textFeaturizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-27ddc260920f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Use TextFeaturizer to generate our features column. We remove stop words, and use TF-IDF to generate 2²⁰ sparse features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocessedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextFeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprocessedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'textFeaturizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Use TextFeaturizer to generate our features column. We remove stop words, and use TF-IDF to generate 2²⁰ sparse features.\n",
    "processedData = textFeaturizer.transform(df)\n",
    "processedData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Text Feauturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  label\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0\n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0\n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0\n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0\n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmlspark.TextFeaturizer import TextFeaturizer\n",
    "textFeaturizer = TextFeaturizer() \\\n",
    "  .setInputCol(\"itemdesc\").setOutputCol(\"features\") \\\n",
    "  .setUseStopWordsRemover(True).setUseIDF(True).setMinDocFreq(5).setNumFeatures(1 << 16).fit(df)\n",
    "processedData = processedData.withColumn(\"label\", processedData[\"label\"]) \\\n",
    "                             .select([\"features\", \"label\"])\n",
    "processedData.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train several Logistic Regression models with different regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train, test, validation = processedData.randomSplit([0.60, 0.20, 0.20])\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from mmlspark.TrainClassifier import TrainClassifier\n",
    "model = TrainClassifier(model=LogisticRegression(),labelCol=\"label\").fit(train)\n",
    "model.write().overwrite().save(\"partsclassification.mml\")\n",
    "#lrHyperParams = [0.05, 0.1, 0.2, 0.4]\n",
    "#logisticRegressions = [LogisticRegression(regParam = hyperParam) for hyperParam in lrHyperParams]\n",
    "\n",
    "\n",
    "#lrmodels = [TrainClassifier(model=lrm, labelCol=\"label\").fit(train) for lrm in logisticRegressions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mmlspark import ComputeModelStatistics, TrainedClassifierModel\n",
    "predictionModel = TrainedClassifierModel.load(\"partsclassification.mml\")\n",
    "prediction = predictionModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_type</th>\n",
       "      <th>predicted_class_as_0.0_actual_is_0.0</th>\n",
       "      <th>predicted_class_as_0.0_actual_is_1.0</th>\n",
       "      <th>predicted_class_as_0.0_actual_is_2.0</th>\n",
       "      <th>predicted_class_as_1.0_actual_is_0.0</th>\n",
       "      <th>predicted_class_as_1.0_actual_is_1.0</th>\n",
       "      <th>predicted_class_as_1.0_actual_is_2.0</th>\n",
       "      <th>predicted_class_as_2.0_actual_is_0.0</th>\n",
       "      <th>predicted_class_as_2.0_actual_is_1.0</th>\n",
       "      <th>predicted_class_as_2.0_actual_is_2.0</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>average_accuracy</th>\n",
       "      <th>macro_averaged_precision</th>\n",
       "      <th>macro_averaged_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Classification</td>\n",
       "      <td>9798.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996849</td>\n",
       "      <td>0.996849</td>\n",
       "      <td>0.996849</td>\n",
       "      <td>0.997899</td>\n",
       "      <td>0.489483</td>\n",
       "      <td>0.481142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  evaluation_type  predicted_class_as_0.0_actual_is_0.0  \\\n",
       "0  Classification                                9798.0   \n",
       "\n",
       "   predicted_class_as_0.0_actual_is_1.0  predicted_class_as_0.0_actual_is_2.0  \\\n",
       "0                                  10.0                                  11.0   \n",
       "\n",
       "   predicted_class_as_1.0_actual_is_0.0  predicted_class_as_1.0_actual_is_1.0  \\\n",
       "0                                   9.0                                   8.0   \n",
       "\n",
       "   predicted_class_as_1.0_actual_is_2.0  predicted_class_as_2.0_actual_is_0.0  \\\n",
       "0                                   0.0                                   1.0   \n",
       "\n",
       "   predicted_class_as_2.0_actual_is_1.0  predicted_class_as_2.0_actual_is_2.0  \\\n",
       "0                                   0.0                                   0.0   \n",
       "\n",
       "   accuracy  precision    recall  average_accuracy  macro_averaged_precision  \\\n",
       "0  0.996849   0.996849  0.996849          0.997899                  0.489483   \n",
       "\n",
       "   macro_averaged_recall  \n",
       "0               0.481142  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ComputeModelStatistics().transform(prediction)\n",
    "metrics.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from mmlspark import FindBestModel, BestModel\n",
    "#bestModel = FindBestModel(evaluationMetric=\"AUC\", models=lrmodels).fit(test)\n",
    "#bestModel.write().overwrite().save(\"model.mml\")\n",
    "#loadedBestModel = BestModel.load(\"model.mml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's accuracy on validation set = 99.80%\n"
     ]
    }
   ],
   "source": [
    "from mmlspark.ComputeModelStatistics import ComputeModelStatistics\n",
    "predictionModel = TrainedClassifierModel.load(\"partsclassification.mml\")\n",
    "predictions = predictionModel.transform(validation)\n",
    "metrics = ComputeModelStatistics().transform(predictions)\n",
    "print(\"Best model's accuracy on validation set = \"\n",
    "      + \"{0:.2f}%\".format(metrics.first()[\"accuracy\"] * 100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
